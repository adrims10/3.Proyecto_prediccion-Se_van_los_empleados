![Clasificacion](https://github.com/user-attachments/assets/316d013a-7709-4531-907a-70d3bded555a)


# Predicci√≥n de Retenci√≥n de Empleados üè¢

Dado una serie de datos, nos enfrentaremos a intentar predecir si un empleado se va a ir de la empresa o no.

En este proyecto, vamos a enfrentarnos a la tarea de desentra√±ar patrones, analizar tendencias y construir un modelo que pueda predecir si un empleado permanecer√° o decidir√° decir adi√≥s seg√∫n una serie de variables dadas.

Vamos a ir comparando varios modelos predictivos para ver cu√°l es el modelo que mejor funciona para acabar montando nuestro predictor.


## üóÇÔ∏è Estructura del Proyecto

    ‚îú‚îÄ‚îÄ notebooks/           # Notebooks de Jupyter donde se encontraran la exploracion de los datos y el                                             preprocesamiento para el train del modelo, el train del mismo y una predicci√≥n.
    ‚îú‚îÄ‚îÄ src/                 # Soportes de funciones.
    ‚îú‚îÄ  Datos                # Datos originales para el estudio del modelo, donde tambien se encuentra un diccionario                                   para enterder las variables que hemos utilizado.
    ‚îú‚îÄ  Datos_pkl            # Datos pkl despues del preprocesamiento de los datos.
    ‚îú‚îÄ‚îÄ README.md            # Descripci√≥n del proyecto.


## üõ†Ô∏è Instalaci√≥n y Requisitos  
Este proyecto utiliza Python 3.12.6.


## üõ†Ô∏è Instalaci√≥n y Requisitos  
Este proyecto utiliza Python 3.12.6.

### **Librer√≠as utilizadas**  
- **pandas**  
  Utilizado para la manipulaci√≥n y an√°lisis de datos.  
  - [Documentaci√≥n de Pandas](https://pandas.pydata.org/pandas-docs/stable/)  

- **numpy**  
  Ayuda en c√°lculos num√©ricos eficientes y matrices multidimensionales.  
  - [Documentaci√≥n de NumPy](https://numpy.org/doc/)  

- **os**  
  Gestiona las operaciones del sistema operativo, como la navegaci√≥n de directorios.  
  - [Documentaci√≥n de os](https://docs.python.org/3/library/os.html)  

- **sys**  
  Permite acceder a variables y funciones espec√≠ficas del int√©rprete de Python.  
  - [Documentaci√≥n de sys](https://docs.python.org/3/library/sys.html)  

- **seaborn**  
  Facilita la creaci√≥n de gr√°ficos estad√≠sticos atractivos y f√°ciles de interpretar.  
  - [Documentaci√≥n de Seaborn](https://seaborn.pydata.org/)  

- **matplotlib.pyplot**  
  Herramienta para crear visualizaciones b√°sicas y avanzadas en Python.  
  - [Documentaci√≥n de Matplotlib](https://matplotlib.org/stable/contents.html)

---

## Conclusiones del Proyecto

### üõ†Ô∏è Preprocesamiento

Durante el preprocesamiento y el estudio de los datos, llegamos a la conclusi√≥n de que hab√≠a 4 variables las cuales no nos iban a afectar a nuestra predicci√≥n puesto que solo era un dato √∫nico. Por lo tanto, decidimos borrar estas variables:  
- N√∫mero de empleados = 1  
- Mayor de 18 a√±os = Yes  
- EmployeeID = Valor √∫nico de identidad del empleado  
- Horas est√°ndar = 8 horas  

Seguimos con la gesti√≥n de nulos y observamos que no tenemos nulos en las variables que de primeras se consideran categ√≥ricas y que tenemos un √≠nfimo porcentaje de nulos en las variables num√©ricas, por lo que imputamos los nulos de las num√©ricas con el m√©todo KNN con los 5 vecinos m√°s cercanos.

El pr√≥ximo paso es identificar los outliers, donde tras observaci√≥n y c√°lculo mediante IQR y √âpsilon DBSCAN, decidimos dejarlos sin tratar puesto que es una informaci√≥n demasiado relevante para el modelo.

Una vez tratados los puntos anteriores, comenzamos a escalar las variables num√©ricas. Como observamos que hay variables categ√≥ricas que se pueden escalar porque son n√∫meros, por ejemplo, nivel de educaci√≥n y m√°s, convertimos a n√∫mero decimal (float). Realizamos el m√©todo Standard Scaler, en este caso porque tenemos variables num√©ricas con mucha diferencia de rangos y con el est√°ndar evitamos que las de mayor rango puedan dominar el modelo afectando as√≠ a su rendimiento.

Pasamos a codificar variables categ√≥ricas, realizando una prueba de chi-cuadrado para ver si hay diferencias entre los grupos y as√≠ codificar con un m√©todo u otro. Usamos Target Encoding para las que no tienen diferencias y One-Hot Encoding para las que s√≠ las tienen en su comparativa con la variable respuesta 'Attrition'.

Por √∫ltimo, observamos el balanceo de la variable respuesta. Vemos un gran desequilibrio hacia el "no se van", por lo que decidimos usar un m√©todo de balanceo SMOTENC. Usamos SMOTENC porque es ideal para datasets mixtos como este, donde hay una combinaci√≥n de variables categ√≥ricas y num√©ricas, y se necesita abordar el problema del desbalanceo de clases.

---

### üõ†Ô∏è Entrenamiento del modelo

Entrenamos el modelo mediante Regresi√≥n Log√≠stica, Decision Tree, Random Forest, Gradient Boosting y XGBoost.  
Nos quedamos con XGBoost para el predictor, puesto que vemos que no hay sobreajuste tanto en el Accuracy (siendo el train 0.92 con un excelente rendimiento y siendo el test de 0.89) como en el AUC (siendo el train de este 0.97, discriminando de manera excelente, y en el test 0.95), lo cual no nos deja se√±ales de overfitting.  

La precisi√≥n, el recall y el Kappa tambi√©n tienen m√©tricas muy buenas para la posible predicci√≥n.

---

### üõ†Ô∏è Predicci√≥n

En el √∫ltimo archivo notebook hemos creado un caso de uso de un empleado con una serie de caracter√≠sticas y hemos realizado una predicci√≥n con nuestro modelo XGBoost entrenado.

---

Si quieres ver m√°s proyectos de Python u otras tecnolog√≠as, no dudes en visitar mi repositorio [adrims10](https://github.com/adrims10).


**Adri√°n Moreno**












